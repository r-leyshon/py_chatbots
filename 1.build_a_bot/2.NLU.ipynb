{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Understanding\n",
    "\n",
    "Starts with some more useful regex patterns:\n",
    "\n",
    "`r\"\\bme\\b\"` will match only the word \"me\"  \n",
    "`[A-Z]{1}[a-z]*` will match any title case word.  \n",
    "\n",
    "If you're going to use a pattern several times, then store it with `re.compile()`.\n",
    "\n",
    "Use of pipe operators within a pattern to match several, also use of `pattern.findall()` for multiple matches within a sentence.\n",
    "\n",
    "***\n",
    "## Flexibly match intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_dict = {\n",
    "    'goodbye': ['see ya', 'bye'],\n",
    "    'greet': [r'\\bhi\\b', 'hola', 'heya'],\n",
    "    'thankyou': ['appreciate', 'thank', r'\\bta\\b']\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'goodbye': re.compile(r'see ya|bye', re.UNICODE),\n",
       " 'greet': re.compile(r'\\bhi\\b|hola|heya', re.UNICODE),\n",
       " 'thankyou': re.compile(r'appreciate|thank|\\bta\\b', re.UNICODE)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile a dict of regex patterns that can look for any of the above pattern matches\n",
    "intent_patterns = {}\n",
    "for key, values in intent_dict.items():\n",
    "    multi_pat = \"|\".join(values)\n",
    "    compiled_pat = re.compile(multi_pat)\n",
    "    # label this flexible pattern with the intent key it came with\n",
    "    intent_patterns[key] = compiled_pat\n",
    "intent_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to find the intent of a message\n",
    "def find_intent(pat_dict, some_input):\n",
    "    matched = None\n",
    "    for intent, patterns in pat_dict.items():\n",
    "        # Check for a pattern match first\n",
    "        if patterns.search(some_input):\n",
    "            matched = intent\n",
    "    return matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greet\n",
      "thankyou\n",
      "goodbye\n"
     ]
    }
   ],
   "source": [
    "print(find_intent(intent_patterns, \"hola! Como estas\"))\n",
    "print(find_intent(intent_patterns, \"thankee sai\"))\n",
    "print(find_intent(intent_patterns, \"see ya bud!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Respond to the intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_dict = {\n",
    "    'default': '...',\n",
    "    'goodbye': 'Have a great day',\n",
    "    'greet': 'Hi there',\n",
    "    'thankyou': \"no problem, that's my job\"\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(string_input, pat_dict, resps):\n",
    "    # get the matched intent\n",
    "    intent = find_intent(pat_dict, string_input.lower())\n",
    "    # Use default as the fll back value\n",
    "    key = \"default\"\n",
    "    if intent in resps:\n",
    "        key = intent\n",
    "    return resps[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Have a great day'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer(\"See ya\", intent_patterns, response_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the wrapper function from module 1 that uses a nice display template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update params to include the lookup dicts as default\n",
    "def user_speaks(user_input, pat_dict=intent_patterns, resps=response_dict, user_format=\"USER:\", bot_format=\"BOT:\"):\n",
    "    \"\"\"Passes the user's input to response handler.\"\"\"\n",
    "    time.sleep(0.6)\n",
    "    print(f\"{user_format} {user_input}\")\n",
    "    # update the line below to use the new flexible match functions\n",
    "    resp = answer(user_input, pat_dict, resps)\n",
    "    time.sleep(0.6)\n",
    "    return f\"{bot_format} {resp}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: Hi hi cherry pie!\n",
      "BOT: Hi there\n",
      "USER: Ta very much my lovely...\n",
      "BOT: no problem, that's my job\n",
      "USER: Gotta go. Bye bye hunny pie.\n",
      "BOT: Have a great day\n"
     ]
    }
   ],
   "source": [
    "print(user_speaks(\"Hi hi cherry pie!\"))\n",
    "print(user_speaks(\"Ta very much my lovely...\"))\n",
    "print(user_speaks(\"Gotta go. Bye bye hunny pie.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Basic NER\n",
    "\n",
    "Named Entity Recognition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_names(string_input):\n",
    "    \"\"\"Searches a string for an indication that a name is being discussed, then search\n",
    "    for a proper noun and return it if found.\"\"\"\n",
    "    # ensure None is returned if no match is found\n",
    "    entity = None\n",
    "    name_pat = re.compile(\"name|call\")\n",
    "    proper_noun_pat = re.compile(\"[A-Z]{1}[a-z]*\")\n",
    "    # look for a sentence about a named entity:\n",
    "    if name_pat.search(string_input):\n",
    "        entity = proper_noun_pat.findall(string_input)\n",
    "        if len(entity) > 0:\n",
    "            # several hits means we need to concatenate values\n",
    "            entity = \" \".join(entity)\n",
    "    return entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jimmy\n",
      "My Jimmy\n"
     ]
    }
   ],
   "source": [
    "print(get_names(\"my name is Jimmy.\"))\n",
    "print(get_names(\"My name is Jimmy.\"))\n",
    "# you can see how this would be limited and won't work with lowering an input string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define respond()\n",
    "def answer_name(str_input):\n",
    "    name = get_names(str_input)\n",
    "    if name is None:\n",
    "        return \"You're mysterious, tell me your name.\"\n",
    "    else:\n",
    "        return f\"Hello, {name}!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update params to include the lookup dicts as default\n",
    "def user_speaks(user_input, pat_dict=intent_patterns, resps=response_dict, user_format=\"USER:\", bot_format=\"BOT:\"):\n",
    "    \"\"\"Passes the user's input to response handler.\"\"\"\n",
    "    time.sleep(0.6)\n",
    "    print(f\"{user_format} {user_input}\")\n",
    "    # update the line below to use the name retrieval funcs\n",
    "    resp = answer_name(user_input)\n",
    "    time.sleep(0.6)\n",
    "    return f\"{bot_format} {resp}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: i am called John Snow\n",
      "BOT: Hello, John Snow!\n",
      "USER: my name is Spartacus\n",
      "BOT: Hello, Spartacus!\n",
      "USER: My name is Spartacus\n",
      "BOT: Hello, My Spartacus!\n"
     ]
    }
   ],
   "source": [
    "print(user_speaks(\"i am called John Snow\"))\n",
    "print(user_speaks(\"my name is Spartacus\"))\n",
    "print(user_speaks(\"My name is Spartacus\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Wordvec with spaCy\n",
    "\n",
    "Great little intro on word vectors, where tokens - floats - are assigned to words, word parts, letters or sentences. These can then be used within ML workflows. spaCy makes several wordvec models available. Here we are using `en_core_web_sm` which is trained upon a large corpus with the GloVe algorithm.\n",
    "\n",
    "Tokens can be compared to others using their cosine similarity:\n",
    "\n",
    "* Vector directions point in same direction = 1\n",
    "* Vector directions are perpindicular = 0 \n",
    "* Vector directions are opposite = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py-chatbots/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_dim = nlp.vocab.vectors_length\n",
    "n_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hey Ho, ah let's go!"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the nlp model on a string to get tokens:\n",
    "doc = nlp(\"Hey Ho, ah let's go!\")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey: [ 2.9      0.48218 -2.2693   0.27522 -7.1124   1.2409  -0.43371]\n",
      "Ho: [-1.9577  -3.629   -4.1803   0.75524  2.439    4.1769  -1.2797 ]\n",
      ",: [-3.3899  -4.7034  -0.56101  1.2291   4.3298  -1.0775  -1.3006 ]\n",
      "ah: [ 3.5059   2.9413  -0.30366 -0.53069 -3.0985   3.9806  -2.8103 ]\n",
      "let: [ 8.0705   6.2403  -5.6268  -0.6813  -3.603    2.8543  -0.82774]\n",
      "'s: [ 3.3163   9.7209  -3.1254  -5.1013  12.248    0.74676 -2.2017 ]\n",
      "go: [ 1.484   8.3944 -8.3806  3.2081 -4.2582  1.9773 -2.7806]\n",
      "!: [ 5.0891  -3.3753  -4.2695  -4.8156   3.8904   6.2171   0.26271]\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(f\"{token}: {token.vector[:7]}\")\n",
    "# showing the first 7 word vectors for the sentence tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc[0].vector) == n_dim\n",
    "# you can see that each token has n_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download ATIS dataset\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "#URL = 'https://raw.githubusercontent.com/jkkummerfeld/text2sql-data/master/data/atis.json'\n",
    "#data = json.loads(requests.get(URL).text)\n",
    "## Flattening JSON data\n",
    "#ATIS = pd.json_normalize(data)\n",
    "#ATIS.head()\n",
    "# This source did not have the labels from what I could see, so going with kaggle instead\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>i want to fly from boston at 838 am and arriv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>what flights are available from pittsburgh to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>atis_flight_time</td>\n",
       "      <td>what is the arrival time in san francisco for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>atis_airfare</td>\n",
       "      <td>cheapest airfare from tacoma to orlando</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>atis_airfare</td>\n",
       "      <td>round trip fares from pittsburgh to philadelp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              label                                               text\n",
       "0       atis_flight   i want to fly from boston at 838 am and arriv...\n",
       "1       atis_flight   what flights are available from pittsburgh to...\n",
       "2  atis_flight_time   what is the arrival time in san francisco for...\n",
       "3      atis_airfare            cheapest airfare from tacoma to orlando\n",
       "4      atis_airfare   round trip fares from pittsburgh to philadelp..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pyprojroot import here\n",
    "# there is a kaggle api, but overkill for this project. Source is:\n",
    "# https://www.kaggle.com/datasets/hassanamin/atis-airlinetravelinformationsystem?resource=download\n",
    "colnames = [\"label\", \"text\"]\n",
    "ATIS = pd.read_csv(os.path.join(here(), \"data\", \"atis_intents_train.csv\"),\n",
    "names = colnames)\n",
    "sentences = ATIS.text\n",
    "n_sent = len(sentences)\n",
    "ATIS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4834, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare a 2D array for storing the vectors\n",
    "import numpy as np\n",
    "X_train = np.zeros((n_sent, n_dim))\n",
    "print(np.shape(X_train))\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0826674 , -0.58716798, -2.1964817 , ..., -1.25990617,\n",
       "        -2.6429491 ,  1.24336207],\n",
       "       [-1.80790913,  1.00080669, -3.05921483, ..., -1.52719319,\n",
       "        -2.36632752,  1.1796701 ],\n",
       "       [-1.36993992,  0.49113077,  0.07254934, ..., -1.3198179 ,\n",
       "        -2.0484488 ,  2.16480947],\n",
       "       ...,\n",
       "       [-1.82230973, -1.06588328, -2.15213466, ..., -2.93899107,\n",
       "        -2.00138211, -1.13957   ],\n",
       "       [-2.00526071,  2.67443204, -2.5772748 , ..., -0.08352997,\n",
       "        -2.17978454,  0.47018856],\n",
       "       [-1.33432257,  3.42474079, -2.36559343, ...,  0.34180367,\n",
       "        -3.09680247,  2.61888719]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pass all the sentences to spacy to calclate the word vectors, storing in our array\n",
    "for row, sentence in enumerate(sentences):\n",
    "    doc = nlp(sentence)\n",
    "    X_train[row, :] = doc.vector\n",
    "X_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Use an SVM to recognise intents\n",
    "\n",
    "Various approaches to cosine similarity are discussed, but the exercises are about fitting a support vector classifier to our data. Data is labelled and with a train test split as is usual, so prepare this now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0            atis_flight\n",
      "1            atis_flight\n",
      "2       atis_flight_time\n",
      "3           atis_airfare\n",
      "4           atis_airfare\n",
      "              ...       \n",
      "4829        atis_airfare\n",
      "4830         atis_flight\n",
      "4831        atis_airline\n",
      "4832         atis_flight\n",
      "4833         atis_flight\n",
      "Name: label, Length: 4834, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4, 4, 5, ..., 3, 4, 4])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode labels as numeric\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "print(ATIS.label)\n",
    "le.fit(ATIS.label)\n",
    "y_train = le.transform(ATIS.label)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Prep the Test Data\n",
    "\n",
    "Import and prep the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>i would like to find a flight from charlotte ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>atis_airfare</td>\n",
       "      <td>on april first i need a ticket from tacoma to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>on april first i need a flight going from pho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>i would like a flight traveling one way from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>i would like a flight from orlando to salt la...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          label                                               text\n",
       "0   atis_flight   i would like to find a flight from charlotte ...\n",
       "1  atis_airfare   on april first i need a ticket from tacoma to...\n",
       "2   atis_flight   on april first i need a flight going from pho...\n",
       "3   atis_flight   i would like a flight traveling one way from ...\n",
       "4   atis_flight   i would like a flight from orlando to salt la..."
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ATIS_test = pd.read_csv(\n",
    "    os.path.join(here(), \"data\", \"atis_intents_test.csv\"), names=colnames)\n",
    "ATIS_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.52020073,  2.37595367, -3.60137939, ...,  0.99911278,\n",
       "        -3.60158348,  1.67940843],\n",
       "       [-1.04470658,  0.23952016, -3.36406946, ..., -0.71620345,\n",
       "        -2.85818815,  1.2970823 ],\n",
       "       [-1.11312497,  2.18487287, -3.68941283, ..., -1.3552922 ,\n",
       "        -3.2125392 ,  0.5888651 ],\n",
       "       ...,\n",
       "       [-1.85663342,  3.25489068, -4.70206499, ...,  1.86979628,\n",
       "        -5.62847376,  0.69890302],\n",
       "       [-1.73054779,  3.26366663, -3.23943543, ...,  0.01536671,\n",
       "        -4.43544579,  2.05688596],\n",
       "       [-0.18429269,  2.82947516, -3.51153922, ...,  1.13745189,\n",
       "        -3.99869776,  1.25371277]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences = ATIS_test.text\n",
    "n_test_sent = len(test_sentences)\n",
    "X_test = np.zeros((n_test_sent, n_dim))\n",
    "# create the word vectors\n",
    "for row, sentence in enumerate(test_sentences):\n",
    "    doc = nlp(sentence)\n",
    "    X_test[row, :] = doc.vector\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Predict Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=1)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC()\n",
    "# fit the model to train data\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 4, 4, 4, 4, 4, 4, 4, 4])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translate the numeric class labels back to text ad check for accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['atis_flight',\n",
       " 'atis_flight',\n",
       " 'atis_flight',\n",
       " 'atis_flight',\n",
       " 'atis_flight',\n",
       " 'atis_flight',\n",
       " 'atis_flight',\n",
       " 'atis_flight',\n",
       " 'atis_flight',\n",
       " 'atis_flight']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_labels = list(le.inverse_transform(y_pred))\n",
    "pred_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy: 96.25 %'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = sum(pred_labels == ATIS_test.label)\n",
    "tot = len(ATIS_test.label)\n",
    "f\"Predicted {correct} labels correctly from a total of {tot} rows\"\n",
    "f\"Accuracy: {round(correct / tot * 100, 2)} %\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('py-chatbots')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dddb5cd0d12278ac213127e29759590790bdd2e1e3f009e6cf1f79712aa83327"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
