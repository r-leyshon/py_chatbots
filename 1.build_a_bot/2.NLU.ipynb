{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Understanding\n",
    "\n",
    "Starts with some more useful regex patterns:\n",
    "\n",
    "`r\"\\bme\\b\"` will match only the word \"me\"  \n",
    "`[A-Z]{1}[a-z]*` will match any title case word.  \n",
    "\n",
    "If you're going to use a pattern several times, then store it with `re.compile()`.\n",
    "\n",
    "Use of pipe operators within a pattern to match several, also use of `pattern.findall()` for multiple matches within a sentence.\n",
    "\n",
    "## Flexibly match intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_dict = {\n",
    "    'goodbye': ['see ya', 'bye'],\n",
    "    'greet': [r'\\bhi\\b', 'hola', 'heya'],\n",
    "    'thankyou': ['appreciate', 'thank', r'\\bta\\b']\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'goodbye': re.compile(r'see ya|bye', re.UNICODE),\n",
       " 'greet': re.compile(r'\\bhi\\b|hola|heya', re.UNICODE),\n",
       " 'thankyou': re.compile(r'appreciate|thank|\\bta\\b', re.UNICODE)}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile a dict of regex patterns that can look for any of the above pattern matches\n",
    "intent_patterns = {}\n",
    "for key, values in intent_dict.items():\n",
    "    multi_pat = \"|\".join(values)\n",
    "    compiled_pat = re.compile(multi_pat)\n",
    "    # label this flexible pattern with the intent key it came with\n",
    "    intent_patterns[key] = compiled_pat\n",
    "intent_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to find the intent of a message\n",
    "def find_intent(pat_dict, some_input):\n",
    "    matched = None\n",
    "    for intent, patterns in pat_dict.items():\n",
    "        # Check for a pattern match first\n",
    "        if patterns.search(some_input):\n",
    "            matched = intent\n",
    "    return matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greet\n",
      "thankyou\n",
      "goodbye\n"
     ]
    }
   ],
   "source": [
    "print(find_intent(intent_patterns, \"hola! Como estas\"))\n",
    "print(find_intent(intent_patterns, \"thankee sai\"))\n",
    "print(find_intent(intent_patterns, \"see ya bud!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Respond to the intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_dict = {\n",
    "    'default': '...',\n",
    "    'goodbye': 'Have a great day',\n",
    "    'greet': 'Hi there',\n",
    "    'thankyou': \"no problem, that's my job\"\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(string_input, pat_dict, resps):\n",
    "    # get the matched intent\n",
    "    intent = find_intent(pat_dict, string_input.lower())\n",
    "    # Use default as the fll back value\n",
    "    key = \"default\"\n",
    "    if intent in resps:\n",
    "        key = intent\n",
    "    return resps[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Have a great day'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer(\"See ya\", intent_patterns, response_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the wrapper function from module 1 that uses a nice display template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update params to include the lookup dicts as default\n",
    "def user_speaks(user_input, pat_dict=intent_patterns, resps=response_dict, user_format=\"USER:\", bot_format=\"BOT:\"):\n",
    "    \"\"\"Passes the user's input to response handler.\"\"\"\n",
    "    time.sleep(0.6)\n",
    "    print(f\"{user_format} {user_input}\")\n",
    "    # update the line below to use the new flexible match functions\n",
    "    resp = answer(user_input, pat_dict, resps)\n",
    "    time.sleep(0.6)\n",
    "    return f\"{bot_format} {resp}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: Hi hi cherry pie!\n",
      "BOT: Hi there\n",
      "USER: Ta very much my lovely...\n",
      "BOT: no problem, that's my job\n",
      "USER: Gotta go. Bye bye hunny pie.\n",
      "BOT: Have a great day\n"
     ]
    }
   ],
   "source": [
    "print(user_speaks(\"Hi hi cherry pie!\"))\n",
    "print(user_speaks(\"Ta very much my lovely...\"))\n",
    "print(user_speaks(\"Gotta go. Bye bye hunny pie.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic NER\n",
    "\n",
    "Named Entity Recognition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_names(string_input):\n",
    "    \"\"\"Searches a string for an indication that a name is being discussed, then search\n",
    "    for a proper noun and return it if found.\"\"\"\n",
    "    # ensure None is returned if no match is found\n",
    "    entity = None\n",
    "    name_pat = re.compile(\"name|call\")\n",
    "    proper_noun_pat = re.compile(\"[A-Z]{1}[a-z]*\")\n",
    "    # look for a sentence about a named entity:\n",
    "    if name_pat.search(string_input):\n",
    "        entity = proper_noun_pat.findall(string_input)\n",
    "        if len(entity) > 0:\n",
    "            # several hits means we need to concatenate values\n",
    "            entity = \" \".join(entity)\n",
    "    return entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jimmy\n",
      "My Jimmy\n"
     ]
    }
   ],
   "source": [
    "print(get_names(\"my name is Jimmy.\"))\n",
    "print(get_names(\"My name is Jimmy.\"))\n",
    "# you can see how this would be limited and won't work with lowering an input string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define respond()\n",
    "def answer_name(str_input):\n",
    "    name = get_names(str_input)\n",
    "    if name is None:\n",
    "        return \"You're mysterious, tell me your name.\"\n",
    "    else:\n",
    "        return f\"Hello, {name}!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update params to include the lookup dicts as default\n",
    "def user_speaks(user_input, pat_dict=intent_patterns, resps=response_dict, user_format=\"USER:\", bot_format=\"BOT:\"):\n",
    "    \"\"\"Passes the user's input to response handler.\"\"\"\n",
    "    time.sleep(0.6)\n",
    "    print(f\"{user_format} {user_input}\")\n",
    "    # update the line below to use the name retrieval funcs\n",
    "    resp = answer_name(user_input)\n",
    "    time.sleep(0.6)\n",
    "    return f\"{bot_format} {resp}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: i am called John Snow\n",
      "BOT: Hello, John Snow!\n",
      "USER: my name is Spartacus\n",
      "BOT: Hello, Spartacus!\n",
      "USER: My name is Spartacus\n",
      "BOT: Hello, My Spartacus!\n"
     ]
    }
   ],
   "source": [
    "print(user_speaks(\"i am called John Snow\"))\n",
    "print(user_speaks(\"my name is Spartacus\"))\n",
    "print(user_speaks(\"My name is Spartacus\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordvec with spaCy\n",
    "\n",
    "Great little intro on word vectors, where tokens - floats - are assigned to words, word parts, letters or sentences. These can then be used within ML workflows. spaCy makes several wordvec models available. Here we are using `en_core_web_sm` which is trained upon a large corpus with the GloVe algorithm.\n",
    "\n",
    "Tokens can be compared to others using their cosine similarity:\n",
    "\n",
    "* Vector directions point in same direction = 1\n",
    "* Vector directions are perpindicular = 0 \n",
    "* Vector directions are opposite = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_dim = nlp.vocab.vectors_length\n",
    "n_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hey Ho, ah let's go!"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the nlp model on a string to get tokens:\n",
    "doc = nlp(\"Hey Ho, ah let's go!\")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey: [ 2.9      0.48218 -2.2693   0.27522 -7.1124   1.2409  -0.43371]\n",
      "Ho: [-1.9577  -3.629   -4.1803   0.75524  2.439    4.1769  -1.2797 ]\n",
      ",: [-3.3899  -4.7034  -0.56101  1.2291   4.3298  -1.0775  -1.3006 ]\n",
      "ah: [ 3.5059   2.9413  -0.30366 -0.53069 -3.0985   3.9806  -2.8103 ]\n",
      "let: [ 8.0705   6.2403  -5.6268  -0.6813  -3.603    2.8543  -0.82774]\n",
      "'s: [ 3.3163   9.7209  -3.1254  -5.1013  12.248    0.74676 -2.2017 ]\n",
      "go: [ 1.484   8.3944 -8.3806  3.2081 -4.2582  1.9773 -2.7806]\n",
      "!: [ 5.0891  -3.3753  -4.2695  -4.8156   3.8904   6.2171   0.26271]\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(f\"{token}: {token.vector[:7]}\")\n",
    "# showing the first 7 word vectors for the sentence tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc[0].vector) == n_dim\n",
    "# you can see that each token has n_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download ATIS dataset\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "URL = 'https://raw.githubusercontent.com/jkkummerfeld/text2sql-data/master/data/atis.json'\n",
    "data = json.loads(requests.get(URL).text)\n",
    "# Flattening JSON data\n",
    "ATIS = pd.json_normalize(data)\n",
    "sentences = []\n",
    "for l in ATIS.sentences:\n",
    "    for d in l:\n",
    "        sentences.append(d[\"text\"])\n",
    "n_sent = len(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5280, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare a 2D array for storing the vectors\n",
    "import numpy as np\n",
    "vec_array = np.zeros((n_sent, n_dim))\n",
    "print(np.shape(vec_array))\n",
    "vec_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.66322637, -1.28980911, -1.05443728, ..., -3.43461752,\n",
       "        -0.70707184,  0.85744095],\n",
       "       [-0.72871637, -0.17177999, -3.97666264, ..., -4.20017004,\n",
       "         0.16950625,  0.9615075 ],\n",
       "       [ 0.55754501,  0.75949955, -2.94400024, ..., -1.26750004,\n",
       "        -4.14976645,  0.34788665],\n",
       "       ...,\n",
       "       [-0.66783941,  2.86714268, -2.67127156, ...,  0.06596395,\n",
       "        -3.84909701,  2.84442091],\n",
       "       [-2.73585796,  3.96805525, -3.66085553, ...,  1.75896692,\n",
       "        -7.27086449,  2.62955999],\n",
       "       [-1.04818916,  3.22615075, -3.07146311, ..., -1.51073062,\n",
       "        -2.75123072,  1.16789699]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pass all the sentences to spacy to calclate the word vectors, storing in our array\n",
    "for row, sentence in enumerate(sentences):\n",
    "    doc = nlp(sentence)\n",
    "    vec_array[row, :] = doc.vector\n",
    "vec_array\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('py-chatbots')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dddb5cd0d12278ac213127e29759590790bdd2e1e3f009e6cf1f79712aa83327"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
