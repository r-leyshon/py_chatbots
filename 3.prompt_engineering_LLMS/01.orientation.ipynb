{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Prompt Engineering with LLMs\n",
    "\n",
    "[This course](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/) is produced by the ubiquitous Andrew Ng & OpenAI Engineer Isa Fulford.\n",
    "\n",
    "The course starts with a distinction in the functionality between 2 types of LLM:\n",
    "\n",
    "**Base LLM**: Goos at predicting the next predicted word, so can exhibit autocomplete\n",
    "behaviour. \n",
    "\n",
    "**Instruction Tuned LLM**: A base LLM trained to respond to instructions such as \n",
    "questions. This often involves **RLHF** - reinforcement learning with human feedback. \n",
    "\n",
    "The LLM output should be:\n",
    "\n",
    "**H**elpful, **H**onest & **H**armless\n",
    "\n",
    "## Prompt Engineering\n",
    "\n",
    "To follow along, you'll need:\n",
    "* to pip install `openai` - this is in the requirements.txt.\n",
    "* to generate an api key from the [openai website](https://platform.openai.com/account/api-keys).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toml\n",
    "import os\n",
    "from pyprojroot import here\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = toml.load(os.path.join(here(), \".ignore_me\", \"secrets.toml\"))\n",
    "PROMPT_ENGINEER = CONFIG[\"openai\"][\"PROMPT_ENGINEER\"]\n",
    "openai.api_key = PROMPT_ENGINEER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(usr_input, mod=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"Get an answer from chat GPT.\n",
    "\n",
    "    Args:\n",
    "        usr_input (str): A prompt generated by the user.\n",
    "        mod (str, optional): The model of GPT to use. Defaults to \"gpt-3.5-turbo\".\n",
    "\n",
    "    Returns:\n",
    "        str: The model response text.\n",
    "    \"\"\"\n",
    "    mod_input = [{\"role\": \"user\", \"content\": usr_input}]\n",
    "    resp = openai.ChatCompletion.create(\n",
    "        model=mod,\n",
    "        messages=mod_input,\n",
    "        temperature=0 # degree of randomness \n",
    "    )\n",
    "    return resp.choices[0].message[\"content\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is not working as rate limits asssociated with my tier of key.\n",
    "\n",
    "## Guidance on Prompts\n",
    "\n",
    "### Use delimeters\n",
    "\n",
    "Longer, more specific prompts are specified. You can use delimeters such as:\n",
    "\n",
    "* `\"\"\"`\n",
    "* ` ``` `\n",
    "* `---`\n",
    "* `<>`\n",
    "* `<tag></tag>` (XML-flavoured tags)\n",
    "\n",
    "Then use the available tags to stucture your prompt as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "war_n_peace  = \"\"\"\n",
    "Some scintillating text tht you wint to summarise  / analyse / do something with \\\n",
    "enburing enouhg context is provided.\n",
    "\"\"\"\n",
    "# now for the instruction\n",
    "usr_input = f\"\"\"Correct any typos in the text delimited by triple backticks\\\n",
    "    ```{war_n_peace}````\n",
    "\"\"\"\n",
    "# get_answer(usr_input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Response generated**:\n",
    "'Some scintillating text that you want to summarise / analyse / do something with ensuring enough context is provided.'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is also a good defence against prompt injection. By exposing the `war_n_peace` input\n",
    "only to the user, chatGPT will be summarising these dynamic inputs rather than following\n",
    "any instructions contained within.\n",
    "\n",
    "### Specify structured output\n",
    "\n",
    "You can ask for HTML, XML, JSON etc, provide enough info for the model to cope though:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Make up 3 new science fiction movies, including their director & tagline. Return\n",
    "format should be JSON with the following keys: movie_name, director_name, tagline.\n",
    "\"\"\"\n",
    "# get_answer(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Response generated**:\n",
    "\n",
    "'{\\n  \"movie1\": {\\n    \"movie_name\": \"Galactic Odyssey\",\\n    \"director_name\": \"Christopher Nolan\",\\n    \"tagline\": \"Journey beyond the stars\"\\n  },\\n  \"movie2\": {\\n    \"movie_name\": \"Chrono Shift\",\\n    \"director_name\": \"Denis Villeneuve\",\\n    \"tagline\": \"Time is not what it seems\"\\n  },\\n  \"movie3\": {\\n    \"movie_name\": \"Quantum Paradox\",\\n    \"director_name\": \"James Cameron\",\\n    \"tagline\": \"The laws of physics are about to be broken\"\\n  }\\n}'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check conditions\n",
    "\n",
    "You can ask the model to check some assumptions prior to making a response. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"soundgarden, queens of the stone age, mark lanegan band\"\n",
    "\n",
    "prompt = f\"\"\" \n",
    "Check whether some well-known rock band names are provided within the \\\n",
    "    backticked-delimiited text:\n",
    "    ```{txt}```\n",
    "    If band names are found, generate a new album name for each band found.\n",
    "    If no band was found, return \\\"No known bands found.\\\" \n",
    "\"\"\"\n",
    "# get_answer(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Response generated**:\n",
    "\n",
    "'New album names:\\n- Soundgarden: \"Echoes of the Black Hole\"\\n- Queens of the Stone Age: \"Villains of the Valley\"\\n- Mark Lanegan Band: \"Phantom Radio Sessions\"\\n\\nNote: As an AI language model, I cannot determine whether these album names already exist or not.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"eggs, ham\"\n",
    "#  get_answer(prompt)\n",
    "# response: 'No known bands found.'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot prompting\n",
    "\n",
    "This is when you provide the model with an exemplar answer. Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "I need you to answer in the style of Yoda:\n",
    "\n",
    "<Luke>: Why can't I lift these stones with the force?\n",
    "<Yoda>: Work hard you must. Easy, it is not.\n",
    "<Luke>: I'm hungry. Can you fix me up a womp rat stew?\n",
    "\"\"\"\n",
    "\n",
    "# get_answer(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Response generated**:  \n",
    "'`<Yoda>`: Hungry, are you? Womp rat stew, I can make. But first, patience you must have. Train your mind and body, you must, to become a Jedi.'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Providing Think Time\n",
    "\n",
    "Asking the model to take longer time can ensure it provides the correct answer rather\n",
    "than quickly returning an incorrect answer. Also, breaking down complex tasks into \n",
    "separate instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"\"\"\"\n",
    "Der Strauß, den ich gepflücket,\n",
    "Grüße dich vieltausendmal!\n",
    "Ich hab mich oft gebücket,\n",
    "Ach, wohl eintausendmal,\n",
    "Und ihn ans Herz gedrücket\n",
    "Wie hunderttausendmal!.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "Perform the following instructions, seperate your answers with carriage returns:\n",
    "1. Translate the backtick-delimited text into American English: ```{txt}```\n",
    "2. Translate the same backtick-delimited text into United Kingdom English.\n",
    "3. Underline the text generated in steps 2 and 3 wherever they differ.\n",
    "4. Summarise the American English version of the text to be as succinct as possible.\n",
    "\n",
    "\"\"\"\n",
    "# get_answer(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Response generated:**\n",
    "\n",
    "'1. \"The bouquet that I picked, greets you a thousand times! I have often bent down, oh, probably a thousand times, and pressed it to my heart like a hundred thousand times!\"\\n2. \"The bouquet that I picked, greets you a thousand times! I have often bent down, oh, probably a thousand times, and pressed it to my heart like a hundred thousand times!\"\\n3. No differences found.\\n4. The speaker greets the recipient with a bouquet that they have picked and expresses how many times they have held it close to their heart.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-chatbots",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
