{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Prompt Engineering with LLMs\n",
    "\n",
    "[This course](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/) is produced by the ubiquitous Andrew Ng & OpenAI Engineer Isa Fulford.\n",
    "\n",
    "The course starts with a distinction in the functionality between 2 types of LLM:\n",
    "\n",
    "**Base LLM**: Goos at predicting the next predicted word, so can exhibit autocomplete\n",
    "behaviour. \n",
    "\n",
    "**Instruction Tuned LLM**: A base LLM trained to respond to instructions such as \n",
    "questions. This often involves **RLHF** - reinforcement learning with human feedback. \n",
    "\n",
    "The LLM output should be:\n",
    "\n",
    "**H**elpful, **H**onest & **H**armless\n",
    "\n",
    "## Prompt Engineering\n",
    "\n",
    "To follow along, you'll need:\n",
    "* to pip install `openai` - this is in the requirements.txt.\n",
    "* to generate an api key from the [openai website](https://platform.openai.com/account/api-keys).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toml\n",
    "import os\n",
    "from pyprojroot import here\n",
    "import openai\n",
    "from redlines import Redlines\n",
    "from IPython.display import display, Markdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = toml.load(os.path.join(here(), \".ignore_me\", \"secrets.toml\"))\n",
    "PROMPT_ENGINEER = CONFIG[\"openai\"][\"PROMPT_ENGINEER\"]\n",
    "openai.api_key = PROMPT_ENGINEER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(usr_input, mod=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"Get an answer from chat GPT.\n",
    "\n",
    "    Args:\n",
    "        usr_input (str): A prompt generated by the user.\n",
    "        mod (str, optional): The model of GPT to use. Defaults to \"gpt-3.5-turbo\".\n",
    "\n",
    "    Returns:\n",
    "        str: The model response text.\n",
    "    \"\"\"\n",
    "    mod_input = [{\"role\": \"user\", \"content\": usr_input}]\n",
    "    resp = openai.ChatCompletion.create(\n",
    "        model=mod,\n",
    "        messages=mod_input,\n",
    "        temperature=0 # degree of randomness \n",
    "    )\n",
    "    return resp.choices[0].message[\"content\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is not working as rate limits asssociated with my tier of key.\n",
    "\n",
    "## Guidance on Prompts\n",
    "\n",
    "### Use delimeters\n",
    "\n",
    "Longer, more specific prompts are specified. You can use delimeters such as:\n",
    "\n",
    "* `\"\"\"`\n",
    "* ` ``` `\n",
    "* `---`\n",
    "* `<>`\n",
    "* `<tag></tag>` (XML-flavoured tags)\n",
    "\n",
    "Then use the available tags to stucture your prompt as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "war_n_peace  = \"\"\"\n",
    "Some scintillating text tht you wint to summarise  / analyse / do something with \\\n",
    "enburing enouhg context is provided.\n",
    "\"\"\"\n",
    "# now for the instruction\n",
    "usr_input = f\"\"\"Correct any typos in the text delimited by triple backticks\\\n",
    "    ```{war_n_peace}````\n",
    "\"\"\"\n",
    "# get_answer(usr_input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Response generated**:\n",
    "'Some scintillating text that you want to summarise / analyse / do something with ensuring enough context is provided.'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is also a good defence against prompt injection. By exposing the `war_n_peace` input\n",
    "only to the user, chatGPT will be summarising these dynamic inputs rather than following\n",
    "any instructions contained within.\n",
    "\n",
    "### Specify structured output\n",
    "\n",
    "You can ask for HTML, XML, JSON etc, provide enough info for the model to cope though:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Make up 3 new science fiction movies, including their director & tagline. Return\n",
    "format should be JSON with the following keys: movie_name, director_name, tagline.\n",
    "\"\"\"\n",
    "# get_answer(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Response generated**:\n",
    "\n",
    "'{\\n  \"movie1\": {\\n    \"movie_name\": \"Galactic Odyssey\",\\n    \"director_name\": \"Christopher Nolan\",\\n    \"tagline\": \"Journey beyond the stars\"\\n  },\\n  \"movie2\": {\\n    \"movie_name\": \"Chrono Shift\",\\n    \"director_name\": \"Denis Villeneuve\",\\n    \"tagline\": \"Time is not what it seems\"\\n  },\\n  \"movie3\": {\\n    \"movie_name\": \"Quantum Paradox\",\\n    \"director_name\": \"James Cameron\",\\n    \"tagline\": \"The laws of physics are about to be broken\"\\n  }\\n}'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check conditions\n",
    "\n",
    "You can ask the model to check some assumptions prior to making a response. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"soundgarden, queens of the stone age, mark lanegan band\"\n",
    "\n",
    "prompt = f\"\"\" \n",
    "Check whether some well-known rock band names are provided within the \\\n",
    "    backticked-delimiited text:\n",
    "    ```{txt}```\n",
    "    If band names are found, generate a new album name for each band found.\n",
    "    If no band was found, return \\\"No known bands found.\\\" \n",
    "\"\"\"\n",
    "# get_answer(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Response generated**:\n",
    "\n",
    "'New album names:\\n- Soundgarden: \"Echoes of the Black Hole\"\\n- Queens of the Stone Age: \"Villains of the Valley\"\\n- Mark Lanegan Band: \"Phantom Radio Sessions\"\\n\\nNote: As an AI language model, I cannot determine whether these album names already exist or not.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"eggs, ham\"\n",
    "#  get_answer(prompt)\n",
    "# response: 'No known bands found.'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot prompting\n",
    "\n",
    "This is when you provide the model with an exemplar answer. Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "I need you to answer in the style of Yoda:\n",
    "\n",
    "<Luke>: Why can't I lift these stones with the force?\n",
    "<Yoda>: Work hard you must. Easy, it is not.\n",
    "<Luke>: I'm hungry. Can you fix me up a womp rat stew?\n",
    "\"\"\"\n",
    "\n",
    "# get_answer(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Response generated**:  \n",
    "'`<Yoda>`: Hungry, are you? Womp rat stew, I can make. But first, patience you must have. Train your mind and body, you must, to become a Jedi.'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Providing Think Time\n",
    "\n",
    "Asking the model to take longer time can ensure it provides the correct answer rather\n",
    "than quickly returning an incorrect answer. Also, breaking down complex tasks into \n",
    "separate instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"\"\"\"\n",
    "Der Strauß, den ich gepflücket,\n",
    "Grüße dich vieltausendmal!\n",
    "Ich hab mich oft gebücket,\n",
    "Ach, wohl eintausendmal,\n",
    "Und ihn ans Herz gedrücket\n",
    "Wie hunderttausendmal!.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "Perform the following instructions, seperate your answers with carriage returns:\n",
    "1. Translate the backtick-delimited text into American English: ```{txt}```\n",
    "2. Translate the same backtick-delimited text into United Kingdom English.\n",
    "3. Underline the text generated in steps 2 and 3 wherever they differ.\n",
    "4. Summarise the American English version of the text to be as succinct as possible.\n",
    "\n",
    "\"\"\"\n",
    "# get_answer(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Response generated:**\n",
    "\n",
    "'1. \"The bouquet that I picked, greets you a thousand times! I have often bent down, oh, probably a thousand times, and pressed it to my heart like a hundred thousand times!\"\\n2. \"The bouquet that I picked, greets you a thousand times! I have often bent down, oh, probably a thousand times, and pressed it to my heart like a hundred thousand times!\"\\n3. No differences found.\\n4. The speaker greets the recipient with a bouquet that they have picked and expresses how many times they have held it close to their heart.'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensuring checks are completed with accuracy\n",
    "\n",
    "Unless you are explicit, it seems the model may skim read and come to the incorrect\n",
    "conclusions. You can be explicit in your instructions to force the model to generate the\n",
    "correct result.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"\"\"The frequence of a wave is the velocity of the wave divided by its wavelength \\\n",
    ". The wavelength of red light is 700 x 10<sup>-9</sup> nm. \\\n",
    "The velocity of light in a vacuum is  3 x 10<sup>8</sup> m/s. \\\n",
    "Calculate the frequency of red light in a vacuum.\n",
    "\n",
    "<student>:\n",
    "700 x 10<sup>9</sup> / 3 x 10<sup>8</sup>\n",
    "= 2.333333333333333e<sup>-1</sup> Hz\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)\n",
    "\n",
    "prompt = f\"\"\"Check whether the student's answer in the following backtick-delimited text\n",
    "is correct: ```{txt}```\n",
    "\"\"\"\n",
    "# get_answer(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model response: Yes, the student's answer is correct.\n",
    "\n",
    "This is incorrect. The student confused the numerator & denominator, forgot the sign in\n",
    "the wavelength power term, got confised with nm to m. To have the model check\n",
    "thoroughly, use a more detailed prompt, such as: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"Check whether the student's answer in the following backtick-delimited text\n",
    "is correct: ```{txt}```.\n",
    "Use the following format:\n",
    "- State the question in equation format\n",
    "- Respond with the student's answer\n",
    "- Respond with your calculated answer. \n",
    "- Compare your answer to the student's answer & determine if they match.\n",
    "- Respond with an outcome and some feedback for the student.\n",
    "\n",
    "\"\"\"\n",
    "# get_answer(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generated response**:\n",
    "\n",
    "Question: What is the frequency of red light in a vacuum?\n",
    "\n",
    "Student's answer: 700 x 10<sup>9</sup> / 3 x 10<sup>8</sup> = 2.333333333333333e<sup>-1</sup> Hz\n",
    "\n",
    "Calculated answer: c = λν, where c = 3 x 10<sup>8</sup> m/s and λ = 700 x 10<sup>-9</sup> m. Therefore, ν = c/λ = (3 x 10<sup>8</sup> m/s)/(700 x 10<sup>-9</sup> m) = 4.285714285714286 x 10<sup>14</sup> Hz.\n",
    "\n",
    "Outcome: The student's answer is incorrect.\n",
    "\n",
    "Feedback: The student made an error in their calculation. They divided the wavelength by the velocity instead of dividing the velocity by the wavelength. They also used nanometers instead of meters for the wavelength. The correct answer is 4.29 x 10<sup>14</sup> Hz. The student should review the formula for calculating frequency and ensure that they are using the correct units.\n",
    "\n",
    "## Limitations\n",
    "\n",
    "The LLM has been trained on a very large corpus, but will not perfectly recall all\n",
    "details. This can result in imaginery responses, known as 'hallucinations'. To try to\n",
    "limit this, have the LLM respond with the relevant text prior to summarising, to trace\n",
    "back the origin of such hallucinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Provide a product description of the following\n",
    "product:\n",
    "Marks & Spencers spam niblets\n",
    "\n",
    "\"\"\"\n",
    "# get_answer(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Unfortunately, as an AI language model, I cannot provide a product description of Marks & Spencers spam niblets as it is not a real product. It is possible that you may have misspelled the product name or it may not exist in the market. Please provide the correct product name or more information about the product so I can assist you better.'\n",
    "\n",
    "I guess the model is too clever for my prompts...\n",
    "\n",
    "The rest of the course is pretty straight forward in terms of describing how to iterate on prompts & summarising strengths of the chatgpt LLM. I will summarise them here and also add in some interesting code snippets introduced within the course. \n",
    "\n",
    "\n",
    "## Summarising\n",
    "\n",
    "The example uses a product review such as those found on ebay. Asks the model to summarise the review within a certain word limit. This is then done in series for several reviews.\n",
    "\n",
    "## Inferring\n",
    "\n",
    "In this chapter, the product review is used to prompt the model for sentiment, topics, item name and brand etc.\n",
    "\n",
    "## Transforming\n",
    "\n",
    "This chapter focussed on translating to different languages. \n",
    "\n",
    "## Expanding \n",
    "\n",
    "This section states that given enough context in the prompt, the LLM are strong in generating stories, poems etc. The code asked the LLM to respond to customer complaints with automated apologies. It also pointed out that each prompt session with the LLM is discrete and context will not be remembered by the LLM between prompts. Therefore it's important to extract perinent details to pass back to the next successive prompt.\n",
    "\n",
    "## Chatbot\n",
    "\n",
    "This section introduces system role messages. These are intended to provide context to the behaviour of the LLM. This is to place limits on its behaviour. For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages =  [  \n",
    "{'role':'system', 'content':'You are a customer service assistant chatbot.'},\n",
    "{'role':'user', 'content':'Hello, I\\'m Miles'},\n",
    "{'role':'assistant', 'content': \"Hi Miles! It's great to meet your acquiantance. \\\n",
    "Can I help you?\"},\n",
    "{'role':'user', 'content':'Yes please, tell me my name?'}  ]\n",
    "#response = get_completion_from_messages(messages, temperature=1)\n",
    "#print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This won't work as the prompt sessions are seperate.\n",
    "\n",
    "The chapter then used the `panel` package to present a text input to the user and present the chat within a UI.\n",
    "\n",
    "ipython display was used to render HTML returned by the LLM. \n",
    "\n",
    "And an interesting packagew as used to show the diff between 2 related pieces of text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The quick brown fox <span style=\"color:red;font-weight:700;text-decoration:line-through;\">jumps over </span><span style=\"color:red;font-weight:700;\">walks past </span>the lazy dog."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jumps = \"The quick brown fox jumps over the lazy dog.\"\n",
    "walks = \"The quick brown fox walks past the lazy dog.\"\n",
    "\n",
    "\n",
    "diff = Redlines(jumps,walks)\n",
    "display(Markdown(diff.output_markdown))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-chatbots",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
